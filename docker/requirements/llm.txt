# LLM Inference Service dependencies
llama-cpp-python>=0.2.0
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
pydantic>=2.0.0
httpx>=0.25.0
