# =============================================================================
# Docker Environment Configuration
# Copy to .env and edit before running: docker compose up
# =============================================================================

# --- LLM Model (REQUIRED) ---
# Absolute path to directory containing your GGUF model file on the host
LLM_MODELS_HOST_DIR=/path/to/your/models
# Filename of the GGUF model inside that directory
LLM_MODEL_FILENAME=model.gguf

# --- LLM Parameters ---
LLM_N_CTX=8192
LLM_N_GPU_LAYERS=-1
LLM_N_THREADS=8
LLM_DEFAULT_MAX_TOKENS=1024
# LLM_DEFAULT_TEMP=0.7      # Uncomment to override auto-detected temperature
LLM_FLASH_ATTN=true
LLM_ENABLE_WARMUP=true

# --- Backend ---
BACKEND_PORT=10821
LOG_LEVEL=INFO
VERBOSE=false

# --- TTS ---
TTS_SAMPLE_RATE=24000
TTS_DEFAULT_VOICE=af_heart
TTS_DEFAULT_LANGUAGE=en

# --- STT ---
STT_MODEL_SIZE=large-v3-turbo
STT_DEVICE=cuda
STT_COMPUTE_TYPE=float16

# --- Service URLs (set automatically by docker-compose, no need to change) ---
# LLM_ENDPOINT=http://llm:8000
# TTS_SERVICE_URL=http://tts:8033
# STT_SERVICE_URL=http://stt:8034
